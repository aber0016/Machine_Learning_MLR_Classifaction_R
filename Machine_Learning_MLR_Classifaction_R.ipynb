{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine_Learning_MLR_Classifaction_R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "## Part 1: Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ar--DpysOWJd"
   },
   "source": [
    "This part is about regression. Specifically,we will be ``predicting the fuel efficiency`` of a car (in kilometers per litre) based on its characteristics. This is a practical problem as Australia is one of the largest automobile markets in the world; thus, correctly predicting the fuel efficiency is necessary to control emission rates to the environment.\n",
    "\n",
    "The dataset has many observations and predictors obtained from many retailers for car models available for sale from 2017 to 2020. The target variable is the fuel efficiency of the car measured in kilometers per litre. The higher this value, the better the fuel efficiency of the car. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"RegressionTrain.csv\")\n",
    "test <- read.csv(\"RegressionTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/RegressionTrain.csv\")\n",
    "test <- read.csv(\"../data/RegressionTest.csv\")\n",
    "label <- read.csv(\"../data/RegressionTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "### Objective 1 \n",
    "\n",
    "Fitting a $\\textbf{multiple linear model}$ to the fuel efficiency data using the ``train`` dataset. By checking the summary information, which predictors/variables are possibly associated with fuel efficiency (use ``0.05`` significant level), and why? Which ``three predictors/variables`` appear to be the strongest predictors of fuel efficiency, and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'Model.Year'</li>\n",
       "\t<li>'Eng.Displacement'</li>\n",
       "\t<li>'No.Cylinders'</li>\n",
       "\t<li>'Aspiration'</li>\n",
       "\t<li>'No.Gears'</li>\n",
       "\t<li>'Lockup.Torque.Converter'</li>\n",
       "\t<li>'Drive.Sys'</li>\n",
       "\t<li>'Max.Ethanol'</li>\n",
       "\t<li>'Fuel.Type'</li>\n",
       "\t<li>'Comb.FE'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'Model.Year'\n",
       "\\item 'Eng.Displacement'\n",
       "\\item 'No.Cylinders'\n",
       "\\item 'Aspiration'\n",
       "\\item 'No.Gears'\n",
       "\\item 'Lockup.Torque.Converter'\n",
       "\\item 'Drive.Sys'\n",
       "\\item 'Max.Ethanol'\n",
       "\\item 'Fuel.Type'\n",
       "\\item 'Comb.FE'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'Model.Year'\n",
       "2. 'Eng.Displacement'\n",
       "3. 'No.Cylinders'\n",
       "4. 'Aspiration'\n",
       "5. 'No.Gears'\n",
       "6. 'Lockup.Torque.Converter'\n",
       "7. 'Drive.Sys'\n",
       "8. 'Max.Ethanol'\n",
       "9. 'Fuel.Type'\n",
       "10. 'Comb.FE'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] \"Model.Year\"              \"Eng.Displacement\"       \n",
       " [3] \"No.Cylinders\"            \"Aspiration\"             \n",
       " [5] \"No.Gears\"                \"Lockup.Torque.Converter\"\n",
       " [7] \"Drive.Sys\"               \"Max.Ethanol\"            \n",
       " [9] \"Fuel.Type\"               \"Comb.FE\"                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "   Model.Year   Eng.Displacement  No.Cylinders    Aspiration    No.Gears     \n",
       " Min.   :2017   Min.   :0.900    Min.   : 3.000   N :630     Min.   : 1.000  \n",
       " 1st Qu.:2017   1st Qu.:2.000    1st Qu.: 4.000   OT:  7     1st Qu.: 6.000  \n",
       " Median :2018   Median :3.000    Median : 6.000   SC: 66     Median : 7.000  \n",
       " Mean   :2018   Mean   :3.144    Mean   : 5.622   TC:685     Mean   : 6.974  \n",
       " 3rd Qu.:2019   3rd Qu.:3.625    3rd Qu.: 6.000   TS: 12     3rd Qu.: 8.000  \n",
       " Max.   :2020   Max.   :8.000    Max.   :16.000              Max.   :10.000  \n",
       " Lockup.Torque.Converter Drive.Sys  Max.Ethanol    Fuel.Type    Comb.FE      \n",
       " N: 325                  4:192     Min.   :10.00   G  :642   Min.   : 4.974  \n",
       " Y:1075                  A:430     1st Qu.:10.00   GM : 16   1st Qu.: 8.591  \n",
       "                         F:372     Median :10.00   GP :363   Median : 9.947  \n",
       "                         P: 37     Mean   :15.11   GPR:379   Mean   :10.447  \n",
       "                         R:369     3rd Qu.:15.00             3rd Qu.:11.756  \n",
       "                                   Max.   :85.00             Max.   :26.224  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get all column names to see which predictors can be used \n",
    "names(train)\n",
    "\n",
    "# get an overview of the possible values each predictor can take\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ ., data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0256 -0.9978 -0.0644  0.7006 11.3941 \n",
       "\n",
       "Coefficients:\n",
       "                           Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              -1.783e+02  8.587e+01  -2.076  0.03809 *  \n",
       "Model.Year                9.640e-02  4.255e-02   2.266  0.02363 *  \n",
       "Eng.Displacement         -1.364e+00  1.025e-01 -13.306  < 2e-16 ***\n",
       "No.Cylinders              4.644e-02  6.769e-02   0.686  0.49282    \n",
       "AspirationOT             -3.452e-01  6.352e-01  -0.543  0.58693    \n",
       "AspirationSC             -9.197e-01  2.282e-01  -4.031 5.85e-05 ***\n",
       "AspirationTC             -1.303e+00  1.288e-01 -10.111  < 2e-16 ***\n",
       "AspirationTS             -1.149e+00  4.945e-01  -2.323  0.02035 *  \n",
       "No.Gears                 -1.307e-01  2.995e-02  -4.364 1.37e-05 ***\n",
       "Lockup.Torque.ConverterY -8.243e-01  1.117e-01  -7.377 2.78e-13 ***\n",
       "Drive.SysA               -8.339e-02  1.521e-01  -0.548  0.58356    \n",
       "Drive.SysF                1.441e+00  1.711e-01   8.419  < 2e-16 ***\n",
       "Drive.SysP               -2.400e-01  2.980e-01  -0.805  0.42087    \n",
       "Drive.SysR                4.328e-02  1.476e-01   0.293  0.76938    \n",
       "Max.Ethanol              -7.076e-03  2.967e-03  -2.385  0.01722 *  \n",
       "Fuel.TypeGM               5.706e-01  4.173e-01   1.368  0.17169    \n",
       "Fuel.TypeGP               4.093e-01  1.369e-01   2.990  0.00284 ** \n",
       "Fuel.TypeGPR              1.363e-01  1.401e-01   0.973  0.33096    \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 1.598 on 1382 degrees of freedom\n",
       "Multiple R-squared:  0.6628,\tAdjusted R-squared:  0.6586 \n",
       "F-statistic: 159.8 on 17 and 1382 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comb.FE is our target variable and we use all the other vraibles as predictors\n",
    "lm.fit <- lm(Comb.FE ~ ., train)\n",
    "\n",
    "# print overview\n",
    "summary(lm.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which predictors/variables are possibly associated with fuel efficiency (use 0.05 significant level), and why? \n",
    "\n",
    "The predictors:\n",
    "\n",
    "- Model.Year\n",
    "- Eng.Displacement\n",
    "- Aspiration\n",
    "- No.Gears\n",
    "- Lockup.Torque.Converter\n",
    "- Drive.Sys\n",
    "- Max.Ethanol\n",
    "- Fuel.Type \n",
    "\n",
    "are possibly associated with fuel efficiency. This is since all of the above listed predictors showed a significance level of less than 0.05. A low significance level below 0.05 is indicative of the data rejecting the null hypothesis of no association between the predictor and the target variable. Thus, we could assume that the predictors we outlined above are associated with our target variable, fuel consumption Comb.FE.\n",
    "\n",
    "\n",
    "### Which three predictors/variables appear to be the strongest predictors of fuel efficiency, and why?\n",
    "\n",
    "The three predictors:\n",
    "\n",
    "- AspirationTC with a p-value of < 2e-16\n",
    "- Eng.Displacement with a p-value of < 2e-16\n",
    "- Drive.Sys with a p-value of < 2e-16\n",
    "\n",
    "These three predictors had the lowest p-values, which provides strong evidence to reject the Null-hypothesis. The Null-hypothesis states that there is no association between the predictor variable and the target variable, fuel efficiency. In the case of us failing to reject the Null hypothesis, we can assume that the predictor value is equal to 0. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "### Objective 2\n",
    "\n",
    "Discussing the effect that the year of manufacture ``(Model.Year)`` variable appears to have on the mean ``fuel efficiency``. Additionally, describe/discuss the effect that the number of gears ``(No.Gears)`` variable has on the mean ``fuel efficiency`` of the car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect that the year of manufacture (Model.Year) variable appears to have on the mean fuel efficiency.\n",
    "\n",
    "The predictor Model.Year had a regression coeficent of 9.640e-02, which translates into 0.0964. This means that when the predictor Model.Year increase by one unit, the target variable, fuel efficiency, increases by 0.0964. \n",
    "\n",
    "Thus, roughly speaking the the newer a car is the more fuel efficient it is. \n",
    "\n",
    "### Effect that the number of gears (No.Gears) variable has on the mean fuel efficiency of the car.\n",
    "\n",
    "The predictor No.Gears had a regression coeficent of -1.307e-01, which translates into -0.1307.This means that when the predictor No.Gears increase by one unit the target variable, fuel efficiency decreased by -0.1307. \n",
    "\n",
    "Thus, roughly speaking the more gears a car has, the less fuel efficeint it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "### Objective 3 \n",
    "\n",
    "Applying the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially less significant variables. Write down the final regression equation obtained after pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = Comb.FE ~ Eng.Displacement + Aspiration + No.Gears + \n",
       "    Lockup.Torque.Converter + Drive.Sys + Max.Ethanol, data = train)\n",
       "\n",
       "Residuals:\n",
       "    Min      1Q  Median      3Q     Max \n",
       "-4.0743 -0.9760 -0.0349  0.6566 11.3971 \n",
       "\n",
       "Coefficients:\n",
       "                          Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)              16.196874   0.282901  57.253  < 2e-16 ***\n",
       "Eng.Displacement         -1.277173   0.043418 -29.416  < 2e-16 ***\n",
       "AspirationOT             -0.100081   0.626276  -0.160 0.873060    \n",
       "AspirationSC             -0.699137   0.213768  -3.271 0.001100 ** \n",
       "AspirationTC             -1.144227   0.107302 -10.664  < 2e-16 ***\n",
       "AspirationTS             -1.122104   0.481471  -2.331 0.019919 *  \n",
       "No.Gears                 -0.113537   0.029183  -3.891 0.000105 ***\n",
       "Lockup.Torque.ConverterY -0.825285   0.110202  -7.489 1.23e-13 ***\n",
       "Drive.SysA                0.035013   0.145617   0.240 0.810020    \n",
       "Drive.SysF                1.480191   0.166847   8.872  < 2e-16 ***\n",
       "Drive.SysP               -0.323201   0.292617  -1.105 0.269560    \n",
       "Drive.SysR                0.093779   0.146329   0.641 0.521710    \n",
       "Max.Ethanol              -0.008344   0.002934  -2.843 0.004528 ** \n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "Residual standard error: 1.605 on 1387 degrees of freedom\n",
       "Multiple R-squared:  0.6586,\tAdjusted R-squared:  0.6557 \n",
       "F-statistic:   223 on 12 and 1387 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the idea behind a stepwise selection procedure is to check which of the parameters\n",
    "# adds value to the model and which don't, thus they will either be added or dropped\n",
    "# in this case the 'value added' of a parameter in the model is measured using\n",
    "# the BIC (Bayesian information criterion) penalty \n",
    "\n",
    "\n",
    "# number of rows in our train dataframe, represents number of observations \n",
    "n = dim(train)[1]\n",
    "\n",
    "# we can fully automate the stepwise selection procedure using the step() function\n",
    "# and providing it with:\n",
    "# the model in use for the current training data set\n",
    "# trace, in how many steps the information should be printed\n",
    "# direction, the direction of the testing, which was specified to both\n",
    "# k, the number of degrees of freedom of the Bayesian information criterion\n",
    "sw.fit <- step(lm.fit,trace = 0,direction = 'both',k = log(n))\n",
    "\n",
    "# display the summary of our newly bulit model using the stepwise selection procedure\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the summary above shows only the parameters:\n",
    "\n",
    "- Eng.Displacement\n",
    "- AspirationOT\n",
    "- AspirationSC\n",
    "- AspirationTC\n",
    "- AspirationTS\n",
    "- No.Gears\n",
    "- Lockup.Torque.ConverterY\n",
    "- Drive.SysA\n",
    "- Drive.SysF\n",
    "- Drive.SysP\n",
    "- Drive.SysR\n",
    "- Max.Ethanol\n",
    "\n",
    "are still in use in the new model sw.fit.\n",
    "\n",
    "Using the provided Intercept of 16.196874 and coefficients for each parameter we can establish the new final regression equation obtained after pruning.\n",
    "\n",
    "E[Comb.FE] = 16.20 - 1.28*Eng.Displacement - 0.10*AspirationO - 1.14*AspirationTC - 1.12*AspirationTS - 0.11*No.Gears - 0.83*Lockup.Torque.ConverterY + 0.04*Drive.SysA + 1.48*Drive.SysF - 0.32*Drive.SysP + 0.09*Drive.SysR - 0.01*Max.Ethanol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izq8RU0lOWJn"
   },
   "source": [
    "### Objective 4 \n",
    "\n",
    "Say we are going to buy a new car and we want to improve the fuel efficiency of our new car, what does this ``BIC model`` suggest we should do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above developed BIC model we know that the predictors' Eng.Displacement, AspirationO, AspirationTC, AspirationTS, 1No.Gears, Lockup.Torque.ConverterY, Drive.SysP, Max.Ethanolare are negatively related to our target variable Comb.FE (fuel efficiency). Since fuel efficiency is measured in in kilometers per litre, predictors which are negatively related to fuel efficiency worsen the fuel efficiency of a car.\n",
    "\n",
    "The predictors' Drive.SysA, Drive.SysF, and Drive.SysR on the other hand are positively related to our target variable Comb.FE (fuel efficiency). Predictors which are positively related to fuel efficiency improve the fuel efficiency of a car.\n",
    "\n",
    "Generally speaking, to improve the fuel efficiency of a new car we would seek to minimize predictors which negatively related to fuel efficiency and maximize predictors which positively related to fuel efficiency.\n",
    "\n",
    "More specifically, we would choose a car with the following features:\n",
    "\n",
    "- a car using Drive.SysF, since it increased fuel efficiency by a factor of 1.4802\n",
    "\n",
    "- a car with a low number of No.Gears, since it decreased fuel efficiency by a factor of  0.1135\n",
    "\n",
    "- a car with a low Lockup.Torque.ConverterY, since it decreased fuel efficiency by a factor of 0.8253\n",
    "\n",
    "- a car with a low AspirationOT, since it decreased fuel efficiency by a factor of 0.1001\n",
    "\n",
    "- a car with a low Eng.Displacement, since it decreased fuel efficiency by a factor of 1.2772\n",
    "\n",
    "- a car with a low Max.Ethanol, since it decreased fuel efficiency by a factor of 0.0083\n",
    "\n",
    "in order to increase fuel efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bhFKnrlcOWJo"
   },
   "source": [
    "### Objective 5\n",
    "\n",
    "We are looking for a new car to buy to replace an existing car. Using the $\\textbf{test}$ dataset to inspect the first car fuel efficiency and see whether it is a good fit for you or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first car to test our model since we are tasked to\n",
    "# use the  test dataset to inspect the first car fuel efficiency \n",
    "# and see whether it is a good fit for you or not.\n",
    "first_car = test[1,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>fit</th><th scope=col>lwr</th><th scope=col>upr</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>9.287257</td><td>9.052956</td><td>9.521557</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lll}\n",
       " fit & lwr & upr\\\\\n",
       "\\hline\n",
       "\t 9.287257 & 9.052956 & 9.521557\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| fit | lwr | upr |\n",
       "|---|---|---|\n",
       "| 9.287257 | 9.052956 | 9.521557 |\n",
       "\n"
      ],
      "text/plain": [
       "  fit      lwr      upr     \n",
       "1 9.287257 9.052956 9.521557"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# now use predict() function to test the predict the \n",
    "# mean fuel efficiency\n",
    "predict(sw.fit,interval = 'confidence',first_car,vcov. = vcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using our BIC model to predict the mean fuel efficiency for this new car. Provide a 95% confidence interval for this prediction. \n",
    "\n",
    "Given the above prediction, we can estimate that the expected mean fuel efficiency of a new car should be 9.29 km per litre (9.287257). Moreover, we are 95% confident that this value will fall between a lower interval value of 9.05 km per litre (9.052956) and\ta higher interval value of 9.52 km per litre (9.521557).\n",
    "\n",
    "\n",
    "b) Given that the current car that you own has a mean fuel efficiency of 9.5 km/l (measured over the lifetime of your ownership), does your model (BIC) suggest that the new car will have better fuel efficiency than your current car? Why?\n",
    "\n",
    "The new car will most likely have better fuel efficiency than your current car. This holds true since we predicted a mean fuel efficiency of 9.29 km per litre (9.287257) and a upper interval value of 9.52 km per litre (9.521557). Thus, using the BIC model, it can be stated that the new car will most likely have a better fuel efficiency than your current car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 6 \n",
    "\n",
    "Save the Planet Regression - Kaggle Competition\n",
    "\n",
    "Predict the fuel efficiency of motor vehicles\n",
    "\n",
    "The [link](https://www.kaggle.com/t/0a3c0fc91b074816a6315bb4e9b42602) to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this function to check the performance of your model\n",
    "rmse <- function(pred.label, truth.label){\n",
    "    # Lower is better\n",
    "    return(sqrt(mean((pred.label - truth.label)^2)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t1400 obs. of  10 variables:\n",
      " $ Model.Year             : int  2020 2020 2020 2020 2020 2020 2020 2020 2020 2020 ...\n",
      " $ Eng.Displacement       : num  1.8 5.2 5.2 3 6.5 5 2 5.2 3.7 5.2 ...\n",
      " $ No.Cylinders           : int  4 10 10 6 12 8 4 10 6 12 ...\n",
      " $ Aspiration             : Factor w/ 5 levels \"N\",\"OT\",\"SC\",..: 4 1 1 4 1 3 4 1 1 4 ...\n",
      " $ No.Gears               : int  6 7 7 8 7 8 8 7 7 8 ...\n",
      " $ Lockup.Torque.Converter: Factor w/ 2 levels \"N\",\"Y\": 2 1 1 2 1 2 2 2 2 2 ...\n",
      " $ Drive.Sys              : Factor w/ 5 levels \"4\",\"A\",\"F\",\"P\",..: 5 2 2 5 5 2 5 2 5 5 ...\n",
      " $ Max.Ethanol            : int  10 15 15 10 10 15 15 15 10 10 ...\n",
      " $ Fuel.Type              : Factor w/ 4 levels \"G\",\"GM\",\"GP\",..: 3 4 4 3 4 3 3 4 4 3 ...\n",
      " $ Comb.FE                : num  12.66 7.23 7.23 11.76 5.88 ...\n"
     ]
    }
   ],
   "source": [
    "# check which varaibles are continous and thus can be used in a multilinear regression\n",
    "# get the names of variables\n",
    "str(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Model.Year   Eng.Displacement  No.Cylinders    Aspiration    No.Gears     \n",
       " Min.   :2017   Min.   :0.900    Min.   : 3.000   N :630     Min.   : 1.000  \n",
       " 1st Qu.:2017   1st Qu.:2.000    1st Qu.: 4.000   OT:  7     1st Qu.: 6.000  \n",
       " Median :2018   Median :3.000    Median : 6.000   SC: 66     Median : 7.000  \n",
       " Mean   :2018   Mean   :3.144    Mean   : 5.622   TC:685     Mean   : 6.974  \n",
       " 3rd Qu.:2019   3rd Qu.:3.625    3rd Qu.: 6.000   TS: 12     3rd Qu.: 8.000  \n",
       " Max.   :2020   Max.   :8.000    Max.   :16.000              Max.   :10.000  \n",
       " Lockup.Torque.Converter Drive.Sys  Max.Ethanol    Fuel.Type    Comb.FE      \n",
       " N: 325                  4:192     Min.   :10.00   G  :642   Min.   : 4.974  \n",
       " Y:1075                  A:430     1st Qu.:10.00   GM : 16   1st Qu.: 8.591  \n",
       "                         F:372     Median :10.00   GP :363   Median : 9.947  \n",
       "                         P: 37     Mean   :15.11   GPR:379   Mean   :10.447  \n",
       "                         R:369     3rd Qu.:15.00             3rd Qu.:11.756  \n",
       "                                   Max.   :85.00             Max.   :26.224  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reuse the 'brute-force approach' shown in tutorial 10 \n",
    "# we use log and squaring transformations and include all available continous predictor variables \n",
    "fullmod <- lm(Comb.FE ~ . + .*. + log(Model.Year) + log(Eng.Displacement) + log(No.Cylinders) + log(No.Gears) + log(Max.Ethanol) \n",
    "              + I(Model.Year^2) + I(Eng.Displacement^2) + I(No.Cylinders^2) + I(No.Gears^2) + I(Max.Ethanol^2)  , \n",
    "              data=train)\n",
    "\n",
    "\n",
    "# lastly we use the step function which \"fine tunes\" the model and only keeps the parmeters that \n",
    "# dd value to the model\n",
    "# this avoids unnecessary complexity \n",
    "fin.mod <- step(fullmod, k = log(1400), trace=0, direction=\"both\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(fin.mod, train[1:9]):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    }
   ],
   "source": [
    "# model rmse comparison - generate predicted values with each model using predict function with the train dataset\n",
    "pred.label.sw.trans <- predict(fin.mod, train[1:9])\n",
    "pred.label.sw <- predict(sw.fit, train[1:9])\n",
    "pred.label.lm<- predict(lm.fit, train[1:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'simple multiple regression rmse:  1.58763251824734'</span>"
      ],
      "text/latex": [
       "'simple multiple regression rmse:  1.58763251824734'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'simple multiple regression rmse:  1.58763251824734'</span>"
      ],
      "text/plain": [
       "[1] \"simple multiple regression rmse:  1.58763251824734\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'stepwise multiple regression rmse:  1.5973496190278'</span>"
      ],
      "text/latex": [
       "'stepwise multiple regression rmse:  1.5973496190278'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'stepwise multiple regression rmse:  1.5973496190278'</span>"
      ],
      "text/plain": [
       "[1] \"stepwise multiple regression rmse:  1.5973496190278\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=white-space:pre-wrap>'complex stepwise multiple regression rmse:  1.3037409137109'</span>"
      ],
      "text/latex": [
       "'complex stepwise multiple regression rmse:  1.3037409137109'"
      ],
      "text/markdown": [
       "<span style=white-space:pre-wrap>'complex stepwise multiple regression rmse:  1.3037409137109'</span>"
      ],
      "text/plain": [
       "[1] \"complex stepwise multiple regression rmse:  1.3037409137109\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# rmse returned by the rmse function with train data set\n",
    "paste('simple multiple regression rmse: ',rmse(pred.label = pred.label.lm, truth.label = train$Comb.FE))\n",
    "paste('stepwise multiple regression rmse: ',rmse(pred.label = pred.label.sw, truth.label = train$Comb.FE))\n",
    "paste('complex stepwise multiple regression rmse: ',rmse(pred.label = pred.label.sw.trans, truth.label = train$Comb.FE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(fin.mod, test):\n",
      "“prediction from a rank-deficient fit may be misleading”"
     ]
    }
   ],
   "source": [
    "# generate predicted Comb.FE using the final model \n",
    "pred.label <- predict(fin.mod, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# put this label in a csv file to commit to the Leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"RegressionPredictLabel.csv\", row.names = F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "RMSE.fin <- rmse(pred.label, label$Label)\n",
    "cat(paste(\"RMSE is\", RMSE.fin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thought process that went into building this model:\n",
    "\n",
    "We created a logistic regression model which, based on inputs such as education level, gender, etc., seeks to determine whether someone is wealthy or not. The process of building a logistic regression model is similar to  multilinear regression model is a continuous trial and error process of making gradual adjustments to the model which enhances its predictive ability.\n",
    "\n",
    "Every multilinear regression model has a base format of:\n",
    "\n",
    "$ \\widehat{y} =  \\beta_0 + \\sum_{j=i}^{p}\\beta_j*x_{i,j} $\n",
    "\n",
    "Given this format, we seek to find the best predictors and transformations that can be applied to the predictors to enhance our model's performance. Performance in this case is measured using mean squared error.\n",
    "\n",
    "Our first step was to identify all variables that can be used as predictor variables for a multilinear regression model. This was done using the str(train) command. Moreover, using the summary(train) command we assess the statistical characteristics of potential predictor variables. After that, we insert all identified variables that can be used as predictor variables into the model. Additionally, we reuse the 'brute-force approach' shown in tutorial 10 - adding predictors after applying logarithms and polynomial degree of 2, as well as interaction between all the predictor variables. In this 'brute-force approach' we 'throw all possible model enhancements at the wall and see what sticks'.\n",
    "\n",
    "After creating such a complex model with many predictors we need to check which of these predictors add value to the model (increase its accuracy). Thus we apply the stepwise selection procedure. The idea behind a stepwise selection procedure is to check which of the parameters adds value to the model and which don't, thus they will either be added or dropped. In this case, the 'value-added' of a parameter in the model is measured using the BIC (Bayesian information criterion) penalty. In R we can fully automate the stepwise selection procedure using the step() function.\n",
    "\n",
    "After creating this model, I compared the RMSE of the simple multiple regression built at Q1 (lm.fit), the multiple regression with stepwise regression at Q3(sw.fit), and my new model with brute-force approach of stepwise regression. The returned RMSE of the new model built had a significantly lower RMSE compared to two other models from above. This was also shown by the rmse score from Kaggle with the test data set. Therefore, I decided to use the new model as my final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-lggSuURze1"
   },
   "source": [
    "# Part 2: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we are going to work with \"Census Income Dataset\" which was originally donated by Ronny Kohavi and Barry Becker to UCI (University of California, Irvine) in 1996. This is a trimmed dataset used for machine learning students to study classification. \n",
    "\n",
    "This dataset has collected over 40,000 records (we excluded some data in our version) regarding personal yearly income with 12 attributes (predictors). The attributes comprise many aspects of a person that may contribute to the yearly income. You can use summary() function to obtain the attributes information. Your prediction task is to determine whether a person makes over 50K a year.\n",
    "\n",
    "We have splitted the dataset into a trainning and a testing set. There are 27245 records in the training set while 13631 records in the testing set. Besides the 12 predictors, there is one more column named Salary indicating whether a person's yearly income is over 50K. The label information is a seperated file for the testing set and will be used by us to asess your performance later. Note the label TRUE means an individual's yearly salary exceeds 50K while FALSE means an individual's yearly salary is under 50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from students' side\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"ClassTrain.csv\")\n",
    "test  <- read.csv(\"ClassTest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Please skip (don't run) this if you are a student\n",
    "# Read in the data from marking tutors' side (ensure no cheating!)\n",
    "remove(list = ls())\n",
    "train <- read.csv(\"../data/ClassTrain.csv\")\n",
    "test  <- read.csv(\"../data/ClassTest.csv\")\n",
    "label <- read.csv(\"../data/ClassTestLabel.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0epC4XmxOWJe"
   },
   "source": [
    "### Objective  1\n",
    "\n",
    "Fit a $\\textbf{Generalized Linear Model (Logistic Regression)}$ to predict level of income (salary) $\\left(\\;\\geq 50\\;\\text{K, or } <50\\;\\text{K}\\;\\right)$ using the ``train`` dataset. Using the results of fitting this model, which predictors do you think are possibly associated with the level of Salary (use ``0.05`` significant level), and why? Which ``three variables`` appear to be the strongest predictors of salary, and why? \n",
    "\n",
    "Furthermore, you can see that you have much more predictors in this part than in the ``linear model`` from Part 1 $\\Rightarrow$ manually checking information is counterproductive. Thus, please write a function to automate these processes $\\textbf{(1)}$ selecting important feature against 0.05 threshold and $\\textbf{(2)}$ Selecting three most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Age                   WorkClass      FinalWeight             Education   \n",
       " Min.   :17.00   Federal-gov     :  874   Min.   :  13769   HS-grad     :9152  \n",
       " 1st Qu.:28.00   Local-gov       : 1947   1st Qu.: 115896   Some-college:6240  \n",
       " Median :37.00   Private         :19936   Median : 176814   Bachelors   :4609  \n",
       " Mean   :38.43   Self-emp-inc    :  990   Mean   : 187061   Masters     :1483  \n",
       " 3rd Qu.:47.00   Self-emp-not-inc: 2294   3rd Qu.: 234096   Assoc-voc   :1228  \n",
       " Max.   :90.00   State-gov       : 1204   Max.   :1484705   11th        : 946  \n",
       "                                                            (Other)     :3587  \n",
       "               MaritalStatus             Occupation           Relationship  \n",
       " Divorced             : 3962   Exec-managerial:3731   Husband       :11346  \n",
       " Married-civ-spouse   :12716   Prof-specialty :3687   Not-in-family : 7052  \n",
       " Married-spouse-absent:  229   Craft-repair   :3662   Other-relative:  628  \n",
       " Never-married        : 8796   Adm-clerical   :3439   Own-child     : 4147  \n",
       " Separated            :  812   Sales          :3355   Unmarried     : 2841  \n",
       " Widowed              :  730   Other-service  :2748   Wife          : 1231  \n",
       "                               (Other)        :6623                         \n",
       "                 Race          Gender       CapitalGain     CapitalLoss     \n",
       " Amer-Indian-Eskimo:  267   Female: 8803   Min.   :    0   Min.   :   0.00  \n",
       " Asian-Pac-Islander:  269   Male  :18442   1st Qu.:    0   1st Qu.:   0.00  \n",
       " Black             : 2583                  Median :    0   Median :   0.00  \n",
       " Other             :  113                  Mean   : 1129   Mean   :  90.39  \n",
       " White             :24013                  3rd Qu.:    0   3rd Qu.:   0.00  \n",
       "                                           Max.   :99999   Max.   :4356.00  \n",
       "                                                                            \n",
       "   HoursWork       Salary       \n",
       " Min.   : 1.00   Mode :logical  \n",
       " 1st Qu.:40.00   FALSE:20266    \n",
       " Median :40.00   TRUE :6979     \n",
       " Mean   :41.03                  \n",
       " 3rd Qu.:45.00                  \n",
       " Max.   :99.00                  \n",
       "                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get an overview of the variables avaiable as parameters and their values\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "“glm.fit: fitted probabilities numerically 0 or 1 occurred”"
     ]
    }
   ],
   "source": [
    "# build a Generalized Linear Model using the glm function \n",
    "# Generalized Linear Models are in use to determine associations and realtionships and \n",
    "# can be used for a variaty of models such Linear Regressions and Logistic Regression\n",
    "glm.fit <- glm(Salary ~., family=binomial, data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ ., family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.1013  -0.5296  -0.1926   0.0276   3.4349  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.614e+00  4.525e-01 -16.826  < 2e-16 ***\n",
       "Age                                 2.626e-02  1.779e-03  14.762  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.214e-01  1.168e-01  -6.179 6.46e-10 ***\n",
       "WorkClassPrivate                   -4.734e-01  9.693e-02  -4.884 1.04e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.974e-01  1.283e-01  -2.317 0.020506 *  \n",
       "WorkClassSelf-emp-not-inc          -9.994e-01  1.139e-01  -8.772  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.757e-01  1.294e-01  -5.996 2.03e-09 ***\n",
       "FinalWeight                         7.896e-07  1.822e-07   4.334 1.46e-05 ***\n",
       "Education11th                       6.909e-02  2.201e-01   0.314 0.753589    \n",
       "Education12th                       5.005e-01  2.940e-01   1.702 0.088676 .  \n",
       "Education7th-8th                   -6.213e-01  2.592e-01  -2.397 0.016530 *  \n",
       "Education9th                       -2.472e-01  2.856e-01  -0.865 0.386877    \n",
       "EducationAssoc-acdm                 1.302e+00  1.843e-01   7.066 1.60e-12 ***\n",
       "EducationAssoc-voc                  1.263e+00  1.772e-01   7.127 1.02e-12 ***\n",
       "EducationBachelors                  1.931e+00  1.647e-01  11.724  < 2e-16 ***\n",
       "EducationDoctorate                  3.076e+00  2.380e-01  12.926  < 2e-16 ***\n",
       "EducationHS-grad                    7.790e-01  1.598e-01   4.874 1.09e-06 ***\n",
       "EducationMasters                    2.319e+00  1.767e-01  13.126  < 2e-16 ***\n",
       "EducationProf-school                2.874e+00  2.145e-01  13.396  < 2e-16 ***\n",
       "EducationSome-college               1.108e+00  1.622e-01   6.832 8.36e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.345e+00  3.050e-01   7.687 1.51e-14 ***\n",
       "MaritalStatusMarried-spouse-absent -3.345e-02  2.697e-01  -0.124 0.901286    \n",
       "MaritalStatusNever-married         -4.513e-01  9.187e-02  -4.912 9.01e-07 ***\n",
       "MaritalStatusSeparated             -9.621e-02  1.733e-01  -0.555 0.578829    \n",
       "MaritalStatusWidowed                1.484e-01  1.656e-01   0.896 0.370163    \n",
       "OccupationCraft-repair              5.906e-02  8.379e-02   0.705 0.480884    \n",
       "OccupationExec-managerial           7.693e-01  8.089e-02   9.511  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.919e-01  1.457e-01  -6.805 1.01e-11 ***\n",
       "OccupationHandlers-cleaners        -7.641e-01  1.529e-01  -4.999 5.77e-07 ***\n",
       "OccupationMachine-op-inspct        -2.794e-01  1.073e-01  -2.605 0.009191 ** \n",
       "OccupationOther-service            -8.967e-01  1.300e-01  -6.900 5.22e-12 ***\n",
       "OccupationProf-specialty            4.654e-01  8.613e-02   5.403 6.54e-08 ***\n",
       "OccupationProtective-serv           6.229e-01  1.302e-01   4.784 1.72e-06 ***\n",
       "OccupationSales                     2.770e-01  8.625e-02   3.211 0.001322 ** \n",
       "OccupationTech-support              6.359e-01  1.159e-01   5.488 4.07e-08 ***\n",
       "OccupationTransport-moving         -1.027e-01  1.032e-01  -0.995 0.319541    \n",
       "RelationshipNot-in-family           6.652e-01  3.021e-01   2.202 0.027683 *  \n",
       "RelationshipOther-relative         -4.067e-01  2.918e-01  -1.394 0.163395    \n",
       "RelationshipOwn-child              -6.044e-01  2.943e-01  -2.054 0.039994 *  \n",
       "RelationshipUnmarried               5.707e-01  3.174e-01   1.798 0.072185 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.071  < 2e-16 ***\n",
       "RaceAsian-Pac-Islander              9.879e-01  2.997e-01   3.296 0.000979 ***\n",
       "RaceBlack                           3.929e-01  2.427e-01   1.619 0.105400    \n",
       "RaceOther                           1.524e-01  4.397e-01   0.347 0.728862    \n",
       "RaceWhite                           5.396e-01  2.305e-01   2.340 0.019266 *  \n",
       "GenderMale                          8.679e-01  8.403e-02  10.328  < 2e-16 ***\n",
       "CapitalGain                         3.191e-04  1.107e-05  28.830  < 2e-16 ***\n",
       "CapitalLoss                         6.503e-04  3.999e-05  16.264  < 2e-16 ***\n",
       "HoursWork                           2.965e-02  1.774e-03  16.714  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17976  on 27196  degrees of freedom\n",
       "AIC: 18074\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then print  summary of our Generalized Linear Model to check which predictors are statistically significant\n",
    "summary(glm.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: please write a function to automate these processes selecting important feature against 0.05\n",
    "\n",
    "# function that only return statistically significant predictors called return_signif_predictors\n",
    "# fucntion has two inputs model, where we input the model in use and threshold, that is used to decide\n",
    "# which p-value we used to determine the signifigance of a factor\n",
    "return_signif_predictors <- function(model,threshold)\n",
    "    \n",
    "{\n",
    "    \n",
    "    # we seek to extract the predictors that have p-values below our threshold \n",
    "    # code to extract predictors based on p-values taken from\n",
    "    # https://stackoverflow.com/questions/23838937/extract-pvalue-from-glm\n",
    "    # user: R. Prost   accessed: 06/11/2020\n",
    "    get_p_value <- coef(summary(model))[,'Pr(>|z|)']\n",
    "    \n",
    "    # keep all the predictors that have p-values below our threshold \n",
    "    p_value_below_threshold <- get_p_value[get_p_value < threshold]\n",
    "    \n",
    "    # don't keep the (Intercept) value since that is not a predictor we can use\n",
    "    p_value_below_threshold <- p_value_below_threshold[labels(p_value_below_threshold)!='(Intercept)']\n",
    "    \n",
    "    # selecting three most important predictors based on their coefficient \n",
    "    # useing rank() function to rank/order predictors and then select the top 3\n",
    "    top_three <- labels(p_value_below_threshold[rank(p_value_below_threshold) %in%1:3])\n",
    "    \n",
    "    # finally, we print all the results of our function\n",
    "    print(paste('The following variables are possibly associated with our target variable the level of Salary:',\n",
    "               paste(labels(p_value_below_threshold),collapse = ', ')))\n",
    "    \n",
    "    print('This is since they all had a p value lower than 0.05, which provides strong evidence to reject the Null-hypothesis.')\n",
    "    \n",
    "    print(paste('The strongest three predictors of salary were the variables:',paste(top_three,collapse=', ')))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The following variables are possibly associated with our target variable the level of Salary: Age, WorkClassLocal-gov, WorkClassPrivate, WorkClassSelf-emp-inc, WorkClassSelf-emp-not-inc, WorkClassState-gov, FinalWeight, Education7th-8th, EducationAssoc-acdm, EducationAssoc-voc, EducationBachelors, EducationDoctorate, EducationHS-grad, EducationMasters, EducationProf-school, EducationSome-college, MaritalStatusMarried-civ-spouse, MaritalStatusNever-married, OccupationExec-managerial, OccupationFarming-fishing, OccupationHandlers-cleaners, OccupationMachine-op-inspct, OccupationOther-service, OccupationProf-specialty, OccupationProtective-serv, OccupationSales, OccupationTech-support, RelationshipNot-in-family, RelationshipOwn-child, RelationshipWife, RaceAsian-Pac-Islander, RaceWhite, GenderMale, CapitalGain, CapitalLoss, HoursWork\"\n",
      "[1] \"This is since they all had a p value lower than 0.05, which provides strong evidence to reject the Null-hypothesis.\"\n",
      "[1] \"The strongest three predictors of salary were the variables: CapitalGain, CapitalLoss, HoursWork\"\n"
     ]
    }
   ],
   "source": [
    "# call the function return_signif_predictors with two inputs \n",
    "return_signif_predictors(model = glm.fit, threshold = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kEUu5eFpOWJi"
   },
   "source": [
    "### Objective 2 \n",
    "\n",
    "Firstly, use the model created in the previous question to predict for the labels of the $\\textbf{train}$ data. Consequently, our objective is to compare this ``predict.label`` with the ``truth.label`` from the $\\textbf{test}$ data. However, as we don't know the $\\textbf{test}$ label, we have to estimate model performance using $\\textbf{train}$ data at this moment.\n",
    "\n",
    "Secondly, since our objective is to estimate the performance of this model in making correct predictions; thus, this question also asks you to explore different [performance metrics](https://en.wikipedia.org/wiki/Precision_and_recall) for classification models. The metrics we will use are $\\textbf{Accuracy, Precision, Recall, and F1 Score}$, please create a function to calculate these value and print them out properly using the given structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply your previous model to perform prediction, keep type = \"response\"\n",
    "# Don't worry if you receive some warnings, they are benign\n",
    "# using the factor function we turn a probability value into a Flase or True Boolean variable\n",
    "# this is our predictor label, whether someone is wealthy or not\n",
    "predict.label <- factor(predict(glm.fit, type=\"response\") > 0.5, c(F,T))\n",
    "\n",
    "# truth label from train data\n",
    "truth.label <- train$Salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model statistics function\n",
    "mod.stat <- function(predict.label, truth.label){\n",
    "    # instantiate the variables\n",
    "    accuracy <- NULL\n",
    "    precision <- NULL\n",
    "    recall <- NULL\n",
    "    F1 <- NULL\n",
    "    \n",
    "    ##############################\n",
    "    #Your calculatation here\n",
    "    \n",
    "    # reuseing code from tutorial week 10 r file\n",
    "    \n",
    "    # create a table that summarizes the true & false negatives as well as the true & false positives\n",
    "    T = table(truth.label, predict.label)\n",
    "    \n",
    "    # calculate the accuracy of our model \n",
    "    accuracy <- round(mean(truth.label==predict.label), digits = 4)\n",
    "    \n",
    "    # calculate the recall of our model \n",
    "    recall <- round(T[2,2]/(T[2,1] + T[2,2]), digits = 4)\n",
    "    \n",
    "    # calculate the precision of our model\n",
    "    precision <- round(T[2,2]/(T[2,2]+T[1,2]), digits = 4)\n",
    "    \n",
    "    # calculate the F1 of our model\n",
    "    F1 <- round(2/(recall^(-1)+precision^(-1)), digits = 4)\n",
    "    \n",
    "    ##############################\n",
    "    \n",
    "    # print the results of our calaculations\n",
    "    return(list(\"accuracy\" = accuracy, \"precision\" = precision, \"recall\" = recall, \"fscore\" = F1))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8452</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7395</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6107</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.669</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8452\n",
       "\\item[\\$precision] 0.7395\n",
       "\\item[\\$recall] 0.6107\n",
       "\\item[\\$fscore] 0.669\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8452\n",
       "$precision\n",
       ":   0.7395\n",
       "$recall\n",
       ":   0.6107\n",
       "$fscore\n",
       ":   0.669\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8452\n",
       "\n",
       "$precision\n",
       "[1] 0.7395\n",
       "\n",
       "$recall\n",
       "[1] 0.6107\n",
       "\n",
       "$fscore\n",
       "[1] 0.669\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the function to get statistics, provide description/discussion after this\n",
    "mod.stat(predict.label, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Discussion:\n",
    "\n",
    "The created classification model seeks to predict whether someone is wealthy or not.\n",
    "The calculated performance metrics for our classification model show that our model has a moderate level of precision, 0.7395 or 74%, and a relatively low level of recall, 0.6107 or 61%. It makes sense for this model to enhance the precision of the model, the accuracy of the model given that our target is to determine if someone is wealthy. Given the high precision of the model, we expected to be able to more accurately predict that someone makes more than 50k per year (is wealthy). The trade off with focusing on the precision of the model is that the recall is likely to be lower. The model's overall F-score is in between these values at 0.669 or 67%.\n",
    "\n",
    "$accuracy\n",
    "0.8452\n",
    "$precision\n",
    "0.7395\n",
    "$recall\n",
    "0.6107\n",
    "$fscore\n",
    "0.669"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XyKtwfOWJj"
   },
   "source": [
    "### Objective 3 \n",
    "\n",
    "Using the stepwise selection procedure with the $\\textbf{BIC}$ penalty to prune out potentially unimportant variables. Checking the performance of your model using the created ``mod.stat()`` function, please give your discussion as how this model is compared with the ``glm.fit``(you can run the ``mod.stat()`` function for this as well if you want to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "glm(formula = Salary ~ Age + WorkClass + FinalWeight + Education + \n",
       "    MaritalStatus + Occupation + Relationship + Gender + CapitalGain + \n",
       "    CapitalLoss + HoursWork, family = binomial, data = train)\n",
       "\n",
       "Deviance Residuals: \n",
       "    Min       1Q   Median       3Q      Max  \n",
       "-5.0961  -0.5291  -0.1936   0.0279   3.4393  \n",
       "\n",
       "Coefficients:\n",
       "                                     Estimate Std. Error z value Pr(>|z|)    \n",
       "(Intercept)                        -7.130e+00  3.929e-01 -18.146  < 2e-16 ***\n",
       "Age                                 2.644e-02  1.778e-03  14.869  < 2e-16 ***\n",
       "WorkClassLocal-gov                 -7.159e-01  1.163e-01  -6.154 7.54e-10 ***\n",
       "WorkClassPrivate                   -4.588e-01  9.626e-02  -4.766 1.88e-06 ***\n",
       "WorkClassSelf-emp-inc              -2.776e-01  1.278e-01  -2.173  0.02977 *  \n",
       "WorkClassSelf-emp-not-inc          -9.857e-01  1.133e-01  -8.703  < 2e-16 ***\n",
       "WorkClassState-gov                 -7.653e-01  1.290e-01  -5.930 3.02e-09 ***\n",
       "FinalWeight                         7.496e-07  1.800e-07   4.164 3.13e-05 ***\n",
       "Education11th                       7.190e-02  2.201e-01   0.327  0.74395    \n",
       "Education12th                       5.065e-01  2.939e-01   1.724  0.08479 .  \n",
       "Education7th-8th                   -6.220e-01  2.593e-01  -2.399  0.01643 *  \n",
       "Education9th                       -2.417e-01  2.851e-01  -0.848  0.39663    \n",
       "EducationAssoc-acdm                 1.323e+00  1.841e-01   7.187 6.60e-13 ***\n",
       "EducationAssoc-voc                  1.277e+00  1.770e-01   7.215 5.38e-13 ***\n",
       "EducationBachelors                  1.947e+00  1.645e-01  11.838  < 2e-16 ***\n",
       "EducationDoctorate                  3.089e+00  2.377e-01  12.998  < 2e-16 ***\n",
       "EducationHS-grad                    7.881e-01  1.596e-01   4.937 7.95e-07 ***\n",
       "EducationMasters                    2.337e+00  1.765e-01  13.239  < 2e-16 ***\n",
       "EducationProf-school                2.894e+00  2.145e-01  13.495  < 2e-16 ***\n",
       "EducationSome-college               1.117e+00  1.621e-01   6.893 5.47e-12 ***\n",
       "MaritalStatusMarried-civ-spouse     2.357e+00  3.045e-01   7.743 9.75e-15 ***\n",
       "MaritalStatusMarried-spouse-absent -3.049e-02  2.690e-01  -0.113  0.90977    \n",
       "MaritalStatusNever-married         -4.482e-01  9.172e-02  -4.886 1.03e-06 ***\n",
       "MaritalStatusSeparated             -1.101e-01  1.728e-01  -0.637  0.52405    \n",
       "MaritalStatusWidowed                1.497e-01  1.655e-01   0.904  0.36583    \n",
       "OccupationCraft-repair              6.350e-02  8.372e-02   0.759  0.44812    \n",
       "OccupationExec-managerial           7.726e-01  8.083e-02   9.558  < 2e-16 ***\n",
       "OccupationFarming-fishing          -9.869e-01  1.456e-01  -6.779 1.21e-11 ***\n",
       "OccupationHandlers-cleaners        -7.678e-01  1.528e-01  -5.026 5.02e-07 ***\n",
       "OccupationMachine-op-inspct        -2.857e-01  1.072e-01  -2.665  0.00770 ** \n",
       "OccupationOther-service            -9.059e-01  1.298e-01  -6.981 2.92e-12 ***\n",
       "OccupationProf-specialty            4.656e-01  8.599e-02   5.414 6.15e-08 ***\n",
       "OccupationProtective-serv           6.192e-01  1.301e-01   4.760 1.94e-06 ***\n",
       "OccupationSales                     2.797e-01  8.618e-02   3.246  0.00117 ** \n",
       "OccupationTech-support              6.444e-01  1.157e-01   5.568 2.57e-08 ***\n",
       "OccupationTransport-moving         -1.082e-01  1.031e-01  -1.050  0.29391    \n",
       "RelationshipNot-in-family           6.761e-01  3.016e-01   2.242  0.02499 *  \n",
       "RelationshipOther-relative         -4.031e-01  2.920e-01  -1.381  0.16742    \n",
       "RelationshipOwn-child              -5.877e-01  2.935e-01  -2.002  0.04525 *  \n",
       "RelationshipUnmarried               5.744e-01  3.168e-01   1.813  0.06984 .  \n",
       "RelationshipWife                    1.332e+00  1.103e-01  12.076  < 2e-16 ***\n",
       "GenderMale                          8.747e-01  8.401e-02  10.412  < 2e-16 ***\n",
       "CapitalGain                         3.184e-04  1.106e-05  28.784  < 2e-16 ***\n",
       "CapitalLoss                         6.509e-04  3.998e-05  16.281  < 2e-16 ***\n",
       "HoursWork                           2.968e-02  1.774e-03  16.735  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n",
       "\n",
       "(Dispersion parameter for binomial family taken to be 1)\n",
       "\n",
       "    Null deviance: 31005  on 27244  degrees of freedom\n",
       "Residual deviance: 17992  on 27200  degrees of freedom\n",
       "AIC: 18082\n",
       "\n",
       "Number of Fisher Scoring iterations: 7\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting to suppress warnings\n",
    "options(warn=-1)\n",
    "\n",
    "# Fit a stepwise model\n",
    "# using the step() function we optimize our model and prune out potentially unimportant variables\n",
    "sw.fit <- step(glm.fit, k = log(27245), direction = 'both', trace =0)\n",
    "\n",
    "# Setting to suppress warnings\n",
    "options(warn=0)\n",
    "\n",
    "# print the summary  of our optimized model to enhance our understand of it\n",
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8451</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7396</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6104</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.6688</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8451\n",
       "\\item[\\$precision] 0.7396\n",
       "\\item[\\$recall] 0.6104\n",
       "\\item[\\$fscore] 0.6688\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8451\n",
       "$precision\n",
       ":   0.7396\n",
       "$recall\n",
       ":   0.6104\n",
       "$fscore\n",
       ":   0.6688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8451\n",
       "\n",
       "$precision\n",
       "[1] 0.7396\n",
       "\n",
       "$recall\n",
       "[1] 0.6104\n",
       "\n",
       "$fscore\n",
       "[1] 0.6688\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Making prediction using train data and view the statistics\n",
    "# using the factor function we turn a probability value into a Flase or True Boolean variable\n",
    "# this is our predictor label, whether someone is wealthy or not\n",
    "predict.label.sw <- factor(predict(sw.fit, type=\"response\")>0.5, c(F,T))\n",
    "\n",
    "# Only run the below if you have labels, in your submission, this must be UNCOMMENTED\n",
    "# checking the performance of your model using the created mod.stat() function\n",
    "mod.stat(predict.label.sw, truth.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The newly created classification model, sw.fit, seeks to predict whether someone is wealthy or not.\n",
    "The new 'optimized model', sw.fit, has not improved performance compared to the old model glm.fit.\n",
    "The new model, sw.fit, has almost identitcal accuracy, precision, recall, and fscore values. Thus regardless of the pruning out of potentially unimportant variables, the model performance did not increase (see below).\n",
    "\n",
    "OLD model: glm.fit\n",
    "$accuracy\n",
    "0.8452\n",
    "$precision\n",
    "0.7395\n",
    "$recall\n",
    "0.6107\n",
    "$fscore\n",
    "0.669\n",
    "\n",
    "NEW model: sw.fit\n",
    "$accuracy\n",
    "0.8451\n",
    "$precision\n",
    "0.7396\n",
    "$recall\n",
    "0.6104\n",
    "$fscore\n",
    "0.6688"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective 4\n",
    "\n",
    "Rich or Not Classification Task - Kaggle Competition\n",
    "\n",
    "Predict whether a person makes more or less than $50k\n",
    "\n",
    "The [link](https://www.kaggle.com/t/1bdebc96607742dbaf47ab36cd3ae421) to the competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Age                   WorkClass      FinalWeight             Education   \n",
       " Min.   :17.00   Federal-gov     :  874   Min.   :  13769   HS-grad     :9152  \n",
       " 1st Qu.:28.00   Local-gov       : 1947   1st Qu.: 115896   Some-college:6240  \n",
       " Median :37.00   Private         :19936   Median : 176814   Bachelors   :4609  \n",
       " Mean   :38.43   Self-emp-inc    :  990   Mean   : 187061   Masters     :1483  \n",
       " 3rd Qu.:47.00   Self-emp-not-inc: 2294   3rd Qu.: 234096   Assoc-voc   :1228  \n",
       " Max.   :90.00   State-gov       : 1204   Max.   :1484705   11th        : 946  \n",
       "                                                            (Other)     :3587  \n",
       "               MaritalStatus             Occupation           Relationship  \n",
       " Divorced             : 3962   Exec-managerial:3731   Husband       :11346  \n",
       " Married-civ-spouse   :12716   Prof-specialty :3687   Not-in-family : 7052  \n",
       " Married-spouse-absent:  229   Craft-repair   :3662   Other-relative:  628  \n",
       " Never-married        : 8796   Adm-clerical   :3439   Own-child     : 4147  \n",
       " Separated            :  812   Sales          :3355   Unmarried     : 2841  \n",
       " Widowed              :  730   Other-service  :2748   Wife          : 1231  \n",
       "                               (Other)        :6623                         \n",
       "                 Race          Gender       CapitalGain     CapitalLoss     \n",
       " Amer-Indian-Eskimo:  267   Female: 8803   Min.   :    0   Min.   :   0.00  \n",
       " Asian-Pac-Islander:  269   Male  :18442   1st Qu.:    0   1st Qu.:   0.00  \n",
       " Black             : 2583                  Median :    0   Median :   0.00  \n",
       " Other             :  113                  Mean   : 1129   Mean   :  90.39  \n",
       " White             :24013                  3rd Qu.:    0   3rd Qu.:   0.00  \n",
       "                                           Max.   :99999   Max.   :4356.00  \n",
       "                                                                            \n",
       "   HoursWork       Salary       \n",
       " Min.   : 1.00   Mode :logical  \n",
       " 1st Qu.:40.00   FALSE:20266    \n",
       " Median :40.00   TRUE :6979     \n",
       " Mean   :41.03                  \n",
       " 3rd Qu.:45.00                  \n",
       " Max.   :99.00                  \n",
       "                                "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assess the statistical characteristics of the predictor variables\n",
    "summary(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example would be use the previous model as your final one\n",
    "# we can re-use our earlier created model sw.fit\n",
    "fin.mod <- sw.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predict label for the TEST data\n",
    "# using the factor function we turn a probability value into a Flase or True Boolean variable\n",
    "# this is our predictor label, whether someone is wealthy or not\n",
    "pred.label <- factor(predict(sw.fit,train[1:12], type=\"response\")>0.5, c(F,T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8451</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.7396</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.6104</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.6688</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8451\n",
       "\\item[\\$precision] 0.7396\n",
       "\\item[\\$recall] 0.6104\n",
       "\\item[\\$fscore] 0.6688\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8451\n",
       "$precision\n",
       ":   0.7396\n",
       "$recall\n",
       ":   0.6104\n",
       "$fscore\n",
       ":   0.6688\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8451\n",
       "\n",
       "$precision\n",
       "[1] 0.7396\n",
       "\n",
       "$recall\n",
       "[1] 0.6104\n",
       "\n",
       "$fscore\n",
       "[1] 0.6688\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mod.stat(pred.label, train$Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl>\n",
       "\t<dt>$accuracy</dt>\n",
       "\t\t<dd>0.8415</dd>\n",
       "\t<dt>$precision</dt>\n",
       "\t\t<dd>0.6853</dd>\n",
       "\t<dt>$recall</dt>\n",
       "\t\t<dd>0.7054</dd>\n",
       "\t<dt>$fscore</dt>\n",
       "\t\t<dd>0.6952</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description}\n",
       "\\item[\\$accuracy] 0.8415\n",
       "\\item[\\$precision] 0.6853\n",
       "\\item[\\$recall] 0.7054\n",
       "\\item[\\$fscore] 0.6952\n",
       "\\end{description}\n"
      ],
      "text/markdown": [
       "$accuracy\n",
       ":   0.8415\n",
       "$precision\n",
       ":   0.6853\n",
       "$recall\n",
       ":   0.7054\n",
       "$fscore\n",
       ":   0.6952\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "$accuracy\n",
       "[1] 0.8415\n",
       "\n",
       "$precision\n",
       "[1] 0.6853\n",
       "\n",
       "$recall\n",
       "[1] 0.7054\n",
       "\n",
       "$fscore\n",
       "[1] 0.6952\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check model performance using threshold of 0.4\n",
    "pred.label <- factor(predict(sw.fit,train[1:12], type=\"response\")>0.4, c(F,T))\n",
    "mod.stat(pred.label, train$Salary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "# Use this csv file to commit to the leaderboard\n",
    "write.csv(data.frame(\"RowIndex\" = seq(1, length(pred.label)), \"Prediction\" = pred.label),  \n",
    "          \"ClassPredictLabel.csv\", row.names = F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLEASE DO NOT ALTER THIS CODE BLOCK\n",
    "## Please skip (don't run) this if you are a student\n",
    "## For teaching team use only\n",
    "source(\"../data/modassess.r\")\n",
    "model.perf <- mod.stat.test(pred.label,label$Label)\n",
    "print(model.perf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result Discussion:\n",
    "\n",
    "The thought process that went into building this model:\n",
    "\n",
    "We created a logistic regression model which, based on inputs such as education level, gender, etc., seeks to determine whether someone is wealthy or not. The first steps in the process of building a logistic regression model are similar to the steps taken in building a multilinear regression model. Using the glm() function we create a Generalised Linear Model which can be used to determine associations and relationships for a variety of models such Linear and Logistic Regressions. \n",
    "\n",
    "This Generalised Linear Model has a base format of:\n",
    "\n",
    "$ v =  \\beta_0 + \\sum_{j=i}^{p}\\beta_j*x_{i,j} $\n",
    "\n",
    "In this case we reused the earlier created model sw.fit. To build that model we sought to find the best predictors and transformations that can be applied to the predictors to enhance our model's performance. Our first step was to identify all variables that can be used as a predictor variable and incorporate them into the model. After creating a Generalised Linear Model using all available predictors we need to check which of these predictors add value to the model. We apply the stepwise selection procedure where we check which of the parameters add value to the model and which don't. Thus parameters will either be added or dropped. In this case, the 'value-added' of a parameter in the model is measured using the BIC (Bayesian information criterion) penalty. In R we can fully automate the stepwise selection procedure using the step() function.\n",
    "\n",
    "\n",
    "At this point the steps to turn a 'regular' regression model into a logistic regression begin. The returned values from glm function are linear predictors, which should be 'normalised' to a value between 0 to 1. To do so, the predict() variable in combination of type attribute specified as 'response' returns the linear predictors after applying sigmoid function $ S(x) = \\frac{1}{1 + e^{-v}}$. \n",
    "\n",
    "After normalising the linear predictors, we set a threshold to apply the binary classification. \n",
    "\n",
    "Using the a combination of the factor() and predict() function we use the earlier constructed sw.fit model to make True False predictions on whether someone is wealthy or not. This binary classification is based on the Sigmoid function which takes the form of:\n",
    "\n",
    "$ S(x) = \\frac{1}{1 + e^{-v}}$\n",
    "\n",
    "To look for an optimal threshold for binary classification, I tried different threshold level between 0.35 to 0.6. However, the accuracy and f-score did not drastically improve based on the difference in threshold level, and I needed to compromise between the f-score and accuracy when changing the threshold. Therefore, the most optimal threshold for the model was defined as 0.5. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 1 Instructions and Contents.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
